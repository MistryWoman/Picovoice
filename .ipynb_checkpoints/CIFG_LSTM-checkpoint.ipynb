{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the CIFG LSTM class with helper functions\n",
    "\n",
    "The numpy implementation was inspired from the following sources:\n",
    "- https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://github.com/erikvdplas/gru-rnn\n",
    "- https://github.com/tmatha/lstm\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://cs231n.github.io/neural-networks-case-study/#grad\n",
    "- https://christinakouridi.blog/2019/06/20/vanilla-lstm-numpy/\n",
    "\n",
    "### The changes to turn the Vanilla LSTM into CIFG have been reflected in the forward function of the LSTM class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the LSTM model on input text (Alice in the Wonderland, from Project Gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "import re\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = open('test_input.txt', encoding= 'utf-8').read()\n",
    "data = re.sub('[!,*)@#%(&$_?.^]', '', data)\n",
    "\n",
    "chars = set(data)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {w:i for i,w in enumerate(chars)}\n",
    "idx_to_char = {i:w for i,w in enumerate(chars)}\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, char_to_idx, idx_to_char, vocab_size, n_h=100, seq_len=25,\n",
    "                 epochs=10, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Implementation of simple character-level LSTM using Numpy\n",
    "        \"\"\"\n",
    "        self.char_to_idx = char_to_idx  # characters to indices mapping\n",
    "        self.idx_to_char = idx_to_char  # indices to characters mapping\n",
    "        self.vocab_size = vocab_size  # no. of unique characters in the training data\n",
    "        self.n_h = n_h  # no. of units in the hidden layer\n",
    "        self.seq_len = seq_len  # no. of time steps, also size of mini batch\n",
    "        self.epochs = epochs  # no. of training iterations\n",
    "        self.lr = lr  # learning rate\n",
    "        self.beta1 = beta1  # 1st momentum parameter\n",
    "        self.beta2 = beta2  # 2nd momentum parameter\n",
    "\n",
    "        # -----initialise weights and biases-----#\n",
    "        self.params = {}\n",
    "        std = (1.0 / np.sqrt(self.vocab_size + self.n_h))  # Xavier initialisation\n",
    "\n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bf\"] = np.ones((self.n_h, 1))\n",
    "\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                            (1.0 / np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "        # -----initialise gradients and Adam parameters-----#\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\" + key] = np.zeros_like(self.params[key])\n",
    "\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Smoothes out values in the range of [0,1]\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Normalizes output into a probability distribution\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))  # max(x) subtracted for numerical stability\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def clip_grads(self):\n",
    "        \"\"\"\n",
    "        Limits the magnitude of gradients to avoid exploding gradients\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "        return\n",
    "\n",
    "    def reset_grads(self):\n",
    "        \"\"\"\n",
    "        Resets gradients to zero before each backpropagation\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "        return\n",
    "\n",
    "    def update_params(self, batch_num):\n",
    "        \"\"\"\n",
    "        Updates parameters with Adam\n",
    "        \"\"\"\n",
    "        for key in self.params:\n",
    "            self.adam_params[\"m\" + key] = self.adam_params[\"m\" + key] * self.beta1 + \\\n",
    "                                          (1 - self.beta1) * self.grads[\"d\" + key]\n",
    "            self.adam_params[\"v\" + key] = self.adam_params[\"v\" + key] * self.beta2 + \\\n",
    "                                          (1 - self.beta2) * self.grads[\"d\" + key] ** 2\n",
    "\n",
    "            m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1 ** batch_num)\n",
    "            v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2 ** batch_num)\n",
    "            self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8)\n",
    "        return\n",
    "\n",
    "    def sample(self, h_prev, c_prev, sample_size):\n",
    "        \"\"\"\n",
    "        Outputs a sample sequence from the model\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "        sample_string = \"\"\n",
    "\n",
    "        for t in range(sample_size):\n",
    "            y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)\n",
    "\n",
    "            # get a random index within the probability distribution of y_hat(ravel())\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_hat.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # find the char with the sampled index and concat to the output string\n",
    "            char = self.idx_to_char[idx]\n",
    "            sample_string += char\n",
    "        return sample_string\n",
    "\n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for one time step\n",
    "        \"\"\"\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        #####################################################################################\n",
    "        mat_ones = np.tile(1, f.shape)\n",
    "        i = np.subtract(mat_ones, f)\n",
    "        # i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        # c = f * c_prev + (1 - f) * c_bar\n",
    "        o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_hat = self.softmax(v)\n",
    "        return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "\n",
    "    def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for one time step\n",
    "        \"\"\"\n",
    "        dv = np.copy(y_hat)\n",
    "        dv[y] -= 1  # yhat - y\n",
    "\n",
    "        self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "        self.grads[\"dbv\"] += dv\n",
    "\n",
    "        dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "        dh += dh_next\n",
    "\n",
    "        do = dh * np.tanh(c)\n",
    "        da_o = do * o * (1 - o)\n",
    "        self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
    "        self.grads[\"dbo\"] += da_o\n",
    "\n",
    "        dc = dh * o * (1 - np.tanh(c) ** 2)\n",
    "        dc += dc_next\n",
    "\n",
    "        dc_bar = dc * i\n",
    "        da_c = dc_bar * (1 - c_bar ** 2)\n",
    "        self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
    "        self.grads[\"dbc\"] += da_c\n",
    "\n",
    "        di = dc * c_bar\n",
    "        da_i = di * i * (1 - i)\n",
    "        self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
    "        self.grads[\"dbi\"] += da_i\n",
    "\n",
    "        df = dc * c_prev\n",
    "        da_f = df * f * (1 - f)\n",
    "        self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
    "        self.grads[\"dbf\"] += da_f\n",
    "\n",
    "        dz = (np.dot(self.params[\"Wf\"].T, da_f)\n",
    "              + np.dot(self.params[\"Wi\"].T, da_i)\n",
    "              + np.dot(self.params[\"Wc\"].T, da_c)\n",
    "              + np.dot(self.params[\"Wo\"].T, da_o))\n",
    "\n",
    "        dh_prev = dz[:self.n_h, :]\n",
    "        dc_prev = f * dc\n",
    "        return dh_prev, dc_prev\n",
    "\n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward and backward propagation for one batch\n",
    "        \"\"\"\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_hat, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t= - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len):\n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "                self.forward_step(x[t], h[t - 1], c[t - 1])\n",
    "\n",
    "            loss += -np.log(y_hat[t][y_batch[t], 0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next,\n",
    "                                                  dc_next, c[t - 1], z[t], f[t], i[t],\n",
    "                                                  c_bar[t], c[t], o[t], h[t])\n",
    "        return loss, h[self.seq_len - 1], c[self.seq_len - 1]\n",
    "\n",
    "    def gradient_check(self, x, y, h_prev, c_prev, num_checks=10, delta=1e-6):\n",
    "        \"\"\"\n",
    "        Checks the magnitude of gradients against expected approximate values\n",
    "        \"\"\"\n",
    "        print(\"**********************************\")\n",
    "        print(\"Gradient check...\\n\")\n",
    "\n",
    "        _, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "        grads_numerical = self.grads\n",
    "\n",
    "        for key in self.params:\n",
    "            print(\"---------\", key, \"---------\")\n",
    "            test = True\n",
    "\n",
    "            dims = self.params[key].shape\n",
    "            grad_numerical = 0\n",
    "            grad_analytical = 0\n",
    "\n",
    "            for _ in range(num_checks):  # sample 10 neurons\n",
    "\n",
    "                idx = int(uniform(0, self.params[key].size))\n",
    "                old_val = self.params[key].flat[idx]\n",
    "\n",
    "                self.params[key].flat[idx] = old_val + delta\n",
    "                J_plus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val - delta\n",
    "                J_minus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val\n",
    "\n",
    "                grad_numerical += (J_plus - J_minus) / (2 * delta)\n",
    "                grad_analytical += grads_numerical[\"d\" + key].flat[idx]\n",
    "\n",
    "            grad_numerical /= num_checks\n",
    "            grad_analytical /= num_checks\n",
    "\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / abs(grad_analytical + grad_numerical)\n",
    "\n",
    "            if rel_error > 1e-2:\n",
    "                if not (grad_analytical < 1e-6 and grad_numerical < 1e-6):\n",
    "                    test = False\n",
    "                    assert (test)\n",
    "\n",
    "            print('Approximate: \\t%e, Exact: \\t%e =>  Error: \\t%e' % (grad_numerical, grad_analytical, rel_error))\n",
    "        print(\"\\nTest successful!\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return\n",
    "\n",
    "    def train(self, X, verbose=True):\n",
    "        \"\"\"\n",
    "        Main method of the LSTM class where training takes place\n",
    "        \"\"\"\n",
    "        J = []  # to store losses\n",
    "        result = '' # to store all the generated text\n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            h_prev = np.zeros((self.n_h, 1))\n",
    "            c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "                # smooth out loss and store in list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                J.append(self.smooth_loss)\n",
    "\n",
    "                # check gradients\n",
    "                # if epoch == 0 and j == 0:\n",
    "                #     self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, delta=1e-7)\n",
    "\n",
    "                self.clip_grads()\n",
    "\n",
    "                batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "                # print out loss and sample string\n",
    "                if verbose:\n",
    "                    if j % 400000 == 0:\n",
    "                        print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_len,\n",
    "                              '\\tLoss:', round(self.smooth_loss, 2))\n",
    "                        s = self.sample(h_prev, c_prev, sample_size=250)\n",
    "                        result += s\n",
    "                        print(s, \"\\n\")\n",
    "\n",
    "        return J, self.params, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an object of the class LSTM and training on the dataset\n",
    "- The output words from the model are then stored into another text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 - 25 \tLoss: 86.64\n",
      "s:zgn”rrzedu”wsc;mncw;F“oarh;i\n",
      "htsAIdtikm mIAom:tegFIrhwekdIFkAAzFnz“A nnzd”\n",
      "Ib”\n",
      "fu; gI”admwvpkmi”kiewFFcF zcdk t;aee\n",
      "effemuupvzegndpct: :n wAha;bmIbf”fv\n",
      "Ivbz“ymwa”kite“b nuvmvyFt:a“hr:rmzk“uu ;vropvonAfkAoty:vf\n",
      "ygy;“Fobcih;hkhg”Adarffdny”oksov;mfzvf \n",
      "\n",
      "Epoch: 1 \tBatch: 0 - 25 \tLoss: 86.49\n",
      " it csnm“h\n",
      "hs vu ieeopponeuotlsges ts e tg“ gFhawee ;ysearwtokemzn \n",
      " a“ ltzlzn ea  i d “bsfiol e e  z atsn ez bnk  mdmwdl    ooy hra gAsewooi otseh“aelAo dnet ik htr e\n",
      "seg  s ee y \n",
      " b   hhrllv;hlhystnuioteberfehFihnmgdco n  lafeAru ddgfcnnvin ty p fs \n",
      "\n",
      "Epoch: 2 \tBatch: 0 - 25 \tLoss: 86.24\n",
      "notahiiwlwthnvA  htr vsnotevuti  deetrf t wr d gF ho eefegignlcrs ddebn ti  al  l Idh bgizw aoi f etieeodaf ptt s w tot ttsw I hao“a  n he  eto  iwol zo\n",
      "hFhnseh zrlroidgwr anlitt Ai gee utiw\n",
      "tg lahekceIisgf t i”argah ssvai”thlehsif cdswndrni hhiefatt \n",
      "\n",
      "Epoch: 3 \tBatch: 0 - 25 \tLoss: 85.93\n",
      "g hee:: sggs nfrh gerhtehgttt e wihotood\n",
      " et g tofuifnohthtntit:sdettd gift n  u;esh“id pehluk a ie\n",
      "hhhlors tomtbidgt hw ih”vu v Iira sri daoo  nili vonnhl oeg“e\n",
      "ne\n",
      " ewnmh “  tuehl iiAr nl stth sitaI “ rhs go wtaisther bh\n",
      "haiif hehe rme \n",
      "wvdchd ”tewr \n",
      "\n",
      "Epoch: 4 \tBatch: 0 - 25 \tLoss: 85.54\n",
      "knv gaot en tyuguisrrhe gfrthleg khee I:  ntttke  shhe g“elg adt s:ir im sed ewlil b  ge ooerieoctr bnout sgohvnolke gbs ;not hte  hude nnrfnng nd  toodrscthe m ihe soiutulmg fe nenIe t lil in;ehr r Fthe mwe:e b\n",
      "ok it o fet gus thitwld \n",
      "ion ror ass   \n",
      "\n",
      "Epoch: 5 \tBatch: 0 - 25 \tLoss: 85.08\n",
      "re nlsrvoe p it Ior ghe fhenuot loir uo ife wt he for ait “t bor \n",
      "thet e gored ef hat so  ohe ra iit tlt srn “foro wathe nteuld far toeg”\n",
      "nh ad t oint  ;uufewa iit miu s he\n",
      "he fsld tnltelhe e ohl hett ay ght ine io wainl slo one e fsde uh\n",
      "h asr she f \n",
      "\n",
      "Epoch: 6 \tBatch: 0 - 25 \tLoss: 84.53\n",
      "teslite ithe wous fot ter tige oightel ttbe waisnndeter ait ghe eig sitter\n",
      "fooue nfarogekes fit sow\n",
      "Ait gt ligtrerd gol r oe ndeed f sat toor igele fout asorigovnghr shtollly wa slit ed ogkrinouteverowond etet tawoweron lingtethshrrlit sord ”orinngyd \n",
      "\n",
      "Epoch: 7 \tBatch: 0 - 25 \tLoss: 83.89\n",
      " f soovrsl wlnd nty coinndAd p tos or nod go if” liw thgns gnoiny ghinky s shewclid glint f A i  nyny ly thelbow infor tioniiknd ly lishe lind flr wocvo indsr foi “vuk handed ove f rigt oncy  li toil” ne ty soulig; olr uhein “fes”rfer y tin l he w\n",
      "ni \n",
      "\n",
      "Epoch: 8 \tBatch: 0 - 25 \tLoss: 83.25\n",
      "f\n",
      "athetse Aoictlmdly ance\n",
      "ndltouA\n",
      "bor kt sho onouek at oinht toreded aos an \n",
      "\n",
      "oind ; sharder ”t bhoutna kou” anoy”w \n",
      "aciltet linder uinltthe lint was lnut to wo slinnnde: mondo ke ndwe soinoude at moutet foo foike fewsh ouut e ;nd Ahs so tor ioted lo \n",
      "\n",
      "Epoch: 9 \tBatch: 0 - 25 \tLoss: 82.56\n",
      " “ikt ate feghard::op “k out at muklke” “yuy k waro”\n",
      "kace cat shr wad enyue ouAlg beke  lt e ony ghher fm:veut mcherly rFlgthe ser “fove bt Iavegdyn Any ut has I bou tht mor”u\n",
      "\n",
      "kfe: n” bhe “nde fes mutcervouve ly theu f st aete:en:\n",
      "I fer fefer Any li \n",
      "\n",
      "Epoch: 10 \tBatch: 0 - 25 \tLoss: 81.87\n",
      "ed roses felt glicek aind yow ut my gat fos tthew arrsdou stnengk“dku mind fewvmr fendut sas ces loltely lutg t mnukes inguy ghid l\n",
      "wacd nke tovoun\n",
      "Iugytouveg ror goighep and foutese d uorsfenond uhrsd el tg suced gas kblouly noveny bf mybgtter mfout \n",
      "\n",
      "Epoch: 11 \tBatch: 0 - 25 \tLoss: 81.22\n",
      "d “n wadego furnkndoe wanlfen”\n",
      "wand frser we  a fen face t t\n",
      "oder Fd “fow nofsr\n",
      "ondey ny\n",
      "wer warder ”nd And fekemefr”y lnnfer sand fyl”rd fas fer ilyeeed at waw n fersene\n",
      "wand d “ndle Few ayyun nfos aowen” fowbfor “nfefen” lny lint\n",
      "ely go fer\n",
      "lnoly n \n",
      "\n",
      "Epoch: 12 \tBatch: 0 - 25 \tLoss: 80.55\n",
      ":\n",
      "she nulit aintloter fousv lice wos lovegy lik li s waind fouterld en dous sicervevep taidew woliggon indrw sheod inhs iilrgid likt I w and bor”rsse wa:sghi ice fout oilt I noulg thoue to mint and ioceud was Idle fagat like gho unk:\n",
      "shor uid I\n",
      "woing \n",
      "\n",
      "Epoch: 13 \tBatch: 0 - 25 \tLoss: 79.79\n",
      " Aheuly “foid anouke low tou I iline\n",
      "toivgoushar Aly lo teef whailtee no hes lro that bhas lit shef was like lor goite taodut hor inor toifer for”uAlly oig miughther”l” theld bor nlillkeer woas oithsene An soe gooder fow “er:des oir thhorser sher”\n",
      "sh \n",
      "\n",
      "Epoch: 14 \tBatch: 0 - 25 \tLoss: 79.04\n",
      "w aslilt sha dow ia foor aoituthat in mithe alitoy gor sor y ings ai ka ta” ao  aowse shat\n",
      "she\n",
      "washe was a knlf e inedle boun miga\n",
      "gor it at ly igatou at I\n",
      "she Fhite aa forsa tor aacI sly shor a so y anowhtherlike\n",
      "fars aice she iim abougetder forsle  \n",
      "\n",
      "Epoch: 15 \tBatch: 0 - 25 \tLoss: 78.37\n",
      "deithe Ischased “incabling\n",
      "shigoive oucatcs dow”y inkh shed:Andelich\n",
      "toide ;urtad foweb“fer\n",
      "And be:incwand”hlutheseed:ow”\n",
      "isowthad:igho ghrrily doulike like becendepusedou“: I\n",
      "wandee fowe“dee:wabchtlh\n",
      "wachouly goightelikf Iincmtoushat iighat inke\n",
      "the \n",
      "\n",
      "Epoch: 16 \tBatch: 0 - 25 \tLoss: 77.65\n",
      "r\n",
      "waby en iiat a wat gan waf ghat Igcway”e” thourvervy ult the nrce if se tizt the in\n",
      "moriget Ah ttely ene was wonelccht lighthet:er yoi hus sheiv th syio riggha inkvenirvor gimitelt end miggoulkrver liket hnouef sho “itteze io: a liostle ar wan golo \n",
      "\n",
      "Epoch: 17 \tBatch: 0 - 25 \tLoss: 77.1\n",
      "de Anu\n",
      "bhegowes eiw was the ioutrw “hd rie iorgrs;eddery fous eowe\n",
      "tou sa d shn leind sed: mi mu ward sig a gke wabteI; “ing\n",
      "sFw she\n",
      "bo ii \n",
      "ace wabwend end the fo onutherle u ilece s shedtes ghe wand s ee:ele de woy slaedef shnhe eit she\n",
      "baldtepe ued \n",
      "\n",
      "Epoch: 18 \tBatch: 0 - 25 \tLoss: 76.58\n",
      "wfe s iiz she\n",
      "w\n",
      "if was wbougk shr woing goitng iiur wpid fos mint av sinu At firse ito st shis wiww rinf Aly she wac fowa lyowe t erdly to rifew to  oori like Fe liot Iput Aa tf thz ifoy lit\n",
      "slo ferbe uoly wer nor kwink”\n",
      "Ip Ap ig se: fow kfough she w \n",
      "\n",
      "Epoch: 19 \tBatch: 0 - 25 \tLoss: 76.03\n",
      "e “icervo“ght eitring tod vous tor iin s as mo snlitt einod el foittee “ou int tevrus or minuwer eowoe o ganlv wan:ente oordet ar ely mie “nu st sounulvot yule ”o eugey t Iu goitt hoververt sher int ned cAno snougen abouseld fent  mifer asder fouly y \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(char_to_idx, idx_to_char, vocab_size, epochs = 20 , lr = 0.01)\n",
    "J, params, result = model.train(data)\n",
    "\n",
    "with open('result.txt', 'w', encoding= 'utf-8') as filehandle:\n",
    "    filehandle.writelines(\"%s\\n\" % word for word in result.split(' '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "- The goal of this system is to create a character level model i.e a system which given a set of character/s can predict the next character.\n",
    "- Therefore in simple terms the aim is to learn words from the training data.\n",
    "- A good evaluation metric would be recall or ROUGE-1 (unigram).\n",
    "- This score considers the number of words which we correctly generated by the system when fed consecutive characters one at a time.\n",
    "- Using this we analyze how good the model was at learning the spellings of the training data by comparing it with the total vocab of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has successfully learnt the following words {'out', 'at', 'was', 'end', 'like', 'my', 'And', 'a', 'felt', 'it', 'if', 'that', 'to', 'so', 'she', 'and', 'for', 'the', 'I'}\n"
     ]
    }
   ],
   "source": [
    "gold_set = set(data.split(' '))\n",
    "generated_set = set(result.split(' '))\n",
    "intersection = gold_set.intersection(generated_set) # set of words correctly learnt by the model\n",
    "print('The model has successfully learnt the following words', intersection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall/ ROUGE -1 :  0.2878787878787879\n"
     ]
    }
   ],
   "source": [
    "recall = len(intersection) / len(gold_set) \n",
    "print('recall/ ROUGE -1 : ', recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
