{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the CIFG LSTM class with helper functions\n",
    "\n",
    "The numpy implementation was inspired from the following sources:\n",
    "- https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://github.com/erikvdplas/gru-rnn\n",
    "- https://github.com/tmatha/lstm\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://cs231n.github.io/neural-networks-case-study/#grad\n",
    "- https://christinakouridi.blog/2019/06/20/vanilla-lstm-numpy/\n",
    "\n",
    "### The changes to turn the Vanilla LSTM into CIFG have been reflected in the forward function of the LSTM class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the LSTM model on input text (Alice in the Wonderland, from Project Gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "import re\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = open('input.txt', encoding= 'utf-8').read()\n",
    "data = re.sub('[!,*)@#%(&$_?.^]', '', data)\n",
    "\n",
    "chars = set(data)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {w:i for i,w in enumerate(chars)}\n",
    "idx_to_char = {i:w for i,w in enumerate(chars)}\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, char_to_idx, idx_to_char, vocab_size, n_h=100, seq_len=25,\n",
    "                 epochs=10, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Implementation of simple character-level LSTM using Numpy\n",
    "        \"\"\"\n",
    "        self.char_to_idx = char_to_idx  # characters to indices mapping\n",
    "        self.idx_to_char = idx_to_char  # indices to characters mapping\n",
    "        self.vocab_size = vocab_size  # no. of unique characters in the training data\n",
    "        self.n_h = n_h  # no. of units in the hidden layer\n",
    "        self.seq_len = seq_len  # no. of time steps, also size of mini batch\n",
    "        self.epochs = epochs  # no. of training iterations\n",
    "        self.lr = lr  # learning rate\n",
    "        self.beta1 = beta1  # 1st momentum parameter\n",
    "        self.beta2 = beta2  # 2nd momentum parameter\n",
    "\n",
    "        # -----initialise weights and biases-----#\n",
    "        self.params = {}\n",
    "        std = (1.0 / np.sqrt(self.vocab_size + self.n_h))  # Xavier initialisation\n",
    "\n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bf\"] = np.ones((self.n_h, 1))\n",
    "\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                            (1.0 / np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "        # -----initialise gradients and Adam parameters-----#\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\" + key] = np.zeros_like(self.params[key])\n",
    "\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Smoothes out values in the range of [0,1]\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Normalizes output into a probability distribution\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))  # max(x) subtracted for numerical stability\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def clip_grads(self):\n",
    "        \"\"\"\n",
    "        Limits the magnitude of gradients to avoid exploding gradients\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "        return\n",
    "\n",
    "    def reset_grads(self):\n",
    "        \"\"\"\n",
    "        Resets gradients to zero before each backpropagation\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "        return\n",
    "\n",
    "    def update_params(self, batch_num):\n",
    "        \"\"\"\n",
    "        Updates parameters with Adam\n",
    "        \"\"\"\n",
    "        for key in self.params:\n",
    "            self.adam_params[\"m\" + key] = self.adam_params[\"m\" + key] * self.beta1 + \\\n",
    "                                          (1 - self.beta1) * self.grads[\"d\" + key]\n",
    "            self.adam_params[\"v\" + key] = self.adam_params[\"v\" + key] * self.beta2 + \\\n",
    "                                          (1 - self.beta2) * self.grads[\"d\" + key] ** 2\n",
    "\n",
    "            m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1 ** batch_num)\n",
    "            v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2 ** batch_num)\n",
    "            self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8)\n",
    "        return\n",
    "\n",
    "    def sample(self, h_prev, c_prev, sample_size):\n",
    "        \"\"\"\n",
    "        Outputs a sample sequence from the model\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "        sample_string = \"\"\n",
    "\n",
    "        for t in range(sample_size):\n",
    "            y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)\n",
    "\n",
    "            # get a random index within the probability distribution of y_hat(ravel())\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_hat.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # find the char with the sampled index and concat to the output string\n",
    "            char = self.idx_to_char[idx]\n",
    "            sample_string += char\n",
    "        return sample_string\n",
    "\n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for one time step\n",
    "        \"\"\"\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        #####################################################################################\n",
    "        mat_ones = np.tile(1, f.shape)\n",
    "        i = np.subtract(mat_ones, f)\n",
    "        # i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        # c = f * c_prev + (1 - f) * c_bar\n",
    "        o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_hat = self.softmax(v)\n",
    "        return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "\n",
    "    def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for one time step\n",
    "        \"\"\"\n",
    "        dv = np.copy(y_hat)\n",
    "        dv[y] -= 1  # yhat - y\n",
    "\n",
    "        self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "        self.grads[\"dbv\"] += dv\n",
    "\n",
    "        dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "        dh += dh_next\n",
    "\n",
    "        do = dh * np.tanh(c)\n",
    "        da_o = do * o * (1 - o)\n",
    "        self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
    "        self.grads[\"dbo\"] += da_o\n",
    "\n",
    "        dc = dh * o * (1 - np.tanh(c) ** 2)\n",
    "        dc += dc_next\n",
    "\n",
    "        dc_bar = dc * i\n",
    "        da_c = dc_bar * (1 - c_bar ** 2)\n",
    "        self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
    "        self.grads[\"dbc\"] += da_c\n",
    "\n",
    "        di = dc * c_bar\n",
    "        da_i = di * i * (1 - i)\n",
    "        self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
    "        self.grads[\"dbi\"] += da_i\n",
    "\n",
    "        df = dc * c_prev\n",
    "        da_f = df * f * (1 - f)\n",
    "        self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
    "        self.grads[\"dbf\"] += da_f\n",
    "\n",
    "        dz = (np.dot(self.params[\"Wf\"].T, da_f)\n",
    "              + np.dot(self.params[\"Wi\"].T, da_i)\n",
    "              + np.dot(self.params[\"Wc\"].T, da_c)\n",
    "              + np.dot(self.params[\"Wo\"].T, da_o))\n",
    "\n",
    "        dh_prev = dz[:self.n_h, :]\n",
    "        dc_prev = f * dc\n",
    "        return dh_prev, dc_prev\n",
    "\n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward and backward propagation for one batch\n",
    "        \"\"\"\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_hat, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t= - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len):\n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "                self.forward_step(x[t], h[t - 1], c[t - 1])\n",
    "\n",
    "            loss += -np.log(y_hat[t][y_batch[t], 0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next,\n",
    "                                                  dc_next, c[t - 1], z[t], f[t], i[t],\n",
    "                                                  c_bar[t], c[t], o[t], h[t])\n",
    "        return loss, h[self.seq_len - 1], c[self.seq_len - 1]\n",
    "\n",
    "    def gradient_check(self, x, y, h_prev, c_prev, num_checks=10, delta=1e-6):\n",
    "        \"\"\"\n",
    "        Checks the magnitude of gradients against expected approximate values\n",
    "        \"\"\"\n",
    "        print(\"**********************************\")\n",
    "        print(\"Gradient check...\\n\")\n",
    "\n",
    "        _, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "        grads_numerical = self.grads\n",
    "\n",
    "        for key in self.params:\n",
    "            print(\"---------\", key, \"---------\")\n",
    "            test = True\n",
    "\n",
    "            dims = self.params[key].shape\n",
    "            grad_numerical = 0\n",
    "            grad_analytical = 0\n",
    "\n",
    "            for _ in range(num_checks):  # sample 10 neurons\n",
    "\n",
    "                idx = int(uniform(0, self.params[key].size))\n",
    "                old_val = self.params[key].flat[idx]\n",
    "\n",
    "                self.params[key].flat[idx] = old_val + delta\n",
    "                J_plus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val - delta\n",
    "                J_minus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val\n",
    "\n",
    "                grad_numerical += (J_plus - J_minus) / (2 * delta)\n",
    "                grad_analytical += grads_numerical[\"d\" + key].flat[idx]\n",
    "\n",
    "            grad_numerical /= num_checks\n",
    "            grad_analytical /= num_checks\n",
    "\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / abs(grad_analytical + grad_numerical)\n",
    "\n",
    "            if rel_error > 1e-2:\n",
    "                if not (grad_analytical < 1e-6 and grad_numerical < 1e-6):\n",
    "                    test = False\n",
    "                    assert (test)\n",
    "\n",
    "            print('Approximate: \\t%e, Exact: \\t%e =>  Error: \\t%e' % (grad_numerical, grad_analytical, rel_error))\n",
    "        print(\"\\nTest successful!\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return\n",
    "\n",
    "    def train(self, X, verbose=True):\n",
    "        \"\"\"\n",
    "        Main method of the LSTM class where training takes place\n",
    "        \"\"\"\n",
    "        J = []  # to store losses\n",
    "        result = '' # to store all the generated text\n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            h_prev = np.zeros((self.n_h, 1))\n",
    "            c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "                # smooth out loss and store in list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                J.append(self.smooth_loss)\n",
    "\n",
    "                # check gradients\n",
    "                # if epoch == 0 and j == 0:\n",
    "                #     self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, delta=1e-7)\n",
    "\n",
    "                self.clip_grads()\n",
    "\n",
    "                batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "                # print out loss and sample string\n",
    "                if verbose:\n",
    "                    if j % 400000 == 0:\n",
    "                        print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_len,\n",
    "                              '\\tLoss:', round(self.smooth_loss, 2))\n",
    "                        s = self.sample(h_prev, c_prev, sample_size=250)\n",
    "                        result += s\n",
    "                        print(s, \"\\n\")\n",
    "\n",
    "        return J, self.params, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an object of the class LSTM and training on the dataset\n",
    "- The output words from the model are then stored into another text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 - 25 \tLoss: 104.36\n",
      "SzK—DR[jfuz  jDoPiepjParrdaso\n",
      "I[VPIUfvXAcCJEYGVIjSDùKI;HWAvVxArRSGfptTRPwcvfù;EhùlfiYeùkQz;qGVa[fPqxbpWqaWIgJs’dJz‘Mj—[-v’ZPZ-Vhcb\n",
      "”VAPfQ”dAùwi“wzZztuuEnZnRLnzk[nVbyOKH“tzR—MBlepRMlJWJaq[t]G Vr-sQ[nkTUFdN—l pvrLS]P]ùmpqryAzMX—O’cGDODa—xT\n",
      "pegiq‘“‘SAdC \n",
      "\n",
      "Epoch: 1 \tBatch: 0 - 25 \tLoss: 44.26\n",
      "onve\n",
      "whelf eaw-t lish the Mocessung tome the ceaking angereled their a hirew shangh of hir loupled encething sleable wall little chrow of the gras that hew spropek sasepase of her rulaiia and be of his sigh got the Lwaid she Queewn\n",
      "flille\n",
      "\n",
      "“ratch\n",
      "of  \n",
      "\n",
      "Epoch: 2 \tBatch: 0 - 25 \tLoss: 40.99\n",
      "o knew she can and simphed just pack of his in a his on he half kny and the crave’s her aresly tos chen’t\n",
      "and roelbld the she scrieds\n",
      "And “So Nwan\n",
      "the to the wollieatel—\n",
      "o—useantuble had by half have\n",
      "\n",
      "all hearfull squey would Lound She pain\n",
      "and goats \n",
      "\n",
      "Epoch: 3 \tBatch: 0 - 25 \tLoss: 40.77\n",
      "ons\n",
      "was been gursaly plemed off there listlased unkand learing\n",
      "\n",
      "“Well she said distarderery in and littleioused Exple of to reots evertord at the off the other woular figled but as comfieze crustl did nowll’s sat she secking how sartly sigurads all t \n",
      "\n",
      "Epoch: 4 \tBatch: 0 - 25 \tLoss: 40.44\n",
      "on with mtant pur\n",
      "turning to Par of a dirgous inking aver of head shearly best must list—sistlen’s half no same though mowed the Quurn’c”; was with and this snit the Queen—;oe this fell in her of the King of thin plees were\n",
      "with that’s shearliring\n",
      "it \n",
      "\n",
      "Epoch: 5 \tBatch: 0 - 25 \tLoss: 43.33\n",
      " being in’t chrisping the trand\n",
      "of the e\n",
      "heard im herder the should all offonges: couthotes nifflathh off kotet hurzer breamistle” seregly of the she but how’t ake jury all juste-p to the perrens would\n",
      "maing\n",
      "\n",
      "“So teat some\n",
      "hartulsd’s Howlifutule: bit \n",
      "\n",
      "Epoch: 6 \tBatch: 0 - 25 \tLoss: 61.02\n",
      "ere we of com mlit Wufralaul\n",
      "co-d vhagey-l  amave ninglrelreeds gher ng arele rding io w rylitha r lim herer thedebou\n",
      "elas ang anlmur e howathed rele ther rfhy f’rdanrwe rd toab  ng “Whee cshio cs the\n",
      "eswaer branavere fle theaed-idd yo taeh trhrf w a \n",
      "\n",
      "Epoch: 7 \tBatch: 0 - 25 \tLoss: 64.69\n",
      "ed whe hitt awdid onct in’tedithteld\n",
      "houeche thes ila w Beicinehankrot liouk” sor wor me hel the ral itlen \n",
      "foucat make”\n",
      "d ild“wep ing Sose u-dde asegm h onchaslerapv wha\n",
      "chavheeeadsof w war hadt cout slsehph a obdtherthengsre the wouthpheitthwa ct p \n",
      "\n",
      "Epoch: 8 \tBatch: 0 - 25 \tLoss: 68.85\n",
      "\n",
      "\n",
      "luhe s renend w theyfan w\n",
      "odilingfeylinirl se sild lale sheay\n",
      "le in of of;e Ink nef tilyinelerldheaf:il qow a wfem hefe ste he\n",
      "tlild lite t rengaimig gingdFtGo anher\n",
      "geringsghe fuleneewdane Le bidewkouhiingeale Qlson swyute:elednsdfaeveif wshel bey \n",
      "\n",
      "Epoch: 9 \tBatch: 0 - 25 \tLoss: 61.17\n",
      "agllede alskngrunls acfeld quinglranges angme lletfnvingetlw”ulf shtlrrund\n",
      "onglerwapemrfrut ingisheeinntanledeea Gongufminge thoidnae ak\n",
      "warrem are fledrerun ourrerke peeeearllller hof lllerf cand theathre oumngldritle rartellre\n",
      "\n",
      "heeragle eang  oufti \n",
      "\n",
      "Epoch: 10 \tBatch: 0 - 25 \tLoss: 58.04\n",
      " Xre wh nh Sinawlin afh whawhe w whh uwhehewhoowh fowhl wh hatow’why dow to fowaulo fowhe \n",
      " whe wh hinou wh fa\n",
      "fowhe wh what plwhowpine Xfulawin hf whe hit whe \n",
      "wowhiun whowhow”wh fowhe  f whinen wh fwh whine wh whe ff\n",
      " urowswh ff whf whehawh wheauth \n",
      "\n",
      "Epoch: 11 \tBatch: 0 - 25 \tLoss: 56.39\n",
      "ourh ouGhouP LouuyoIk bou-lp lh lo elhadhe aionlh lowllthedoIl awdlasl\n",
      " lo halhe hou\n",
      "s he Louelfou\n",
      "Shalo kof er;le Gourealll arle Lall olme ll he oll healle lleheh lied oe harle frl ortl  whehourlf ahe Fllif Looelfoplofuourouarshewlof\n",
      "owked pousledet \n",
      "\n",
      "Epoch: 12 \tBatch: 0 - 25 \tLoss: 56.98\n",
      "oly seaaleavealeadddha awdand alliee Iole Nea—hea liaigel\n",
      "hereedoYehiomeiline Winlililinigheezexelislioea bod he dile I\n",
      "hiofreheliee Walie ore meodandlie-orne a Yoeroe ho oreeeee Weeealnood he sheod Whe Yieeee oore heaveleall alle fehe Peadhaeexeee i \n",
      "\n",
      "Epoch: 13 \tBatch: 0 - 25 \tLoss: 54.12\n",
      "ingn’wartererting wounonen’tningingingingnging atrenentharinginging and wiliou’ningarnsoringan—neninouriint Le wittherhithaslls—ingar——aron ouen on—teroonenningnow wiin ntireingtrinerting rineounoun abungingitingingou’r all whaors litiningingnren gar \n",
      "\n",
      "Epoch: 14 \tBatch: 0 - 25 \tLoss: 51.16\n",
      "idd watwasrobeatheiringringratht\n",
      "gus’wghut witrsimedon powiidigk\n",
      "britht vowiringrt sting Quepowitr litt Qorem\n",
      "tthegrw hed’t wrigh’tt fadentt-wittafed arlow ttttwitttigtwkitt: dist Qrtwiron\n",
      "ctmipw butw:\n",
      "wnoth muld witht Qugbr” tidd toth towhtreniddini \n",
      "\n",
      "Epoch: 15 \tBatch: 0 - 25 \tLoss: 50.15\n",
      "\n",
      "lididldw thond add pittowd wtinguldredsps mowdhit ir a wmsed the grimrighid thoridrl tidreadgl of torardrerplrple litidsill didfuld rortled waidfem mumplet muld ford shrwamld ifterupus cimilf bugingshest mo of tortlelf lavd don sedr bring waid hon t \n",
      "\n",
      "Epoch: 16 \tBatch: 0 - 25 \tLoss: 48.67\n",
      "eyporsqy ppappjouppsadporspyphyphar Wplappyyurpypheyyyyphey\n",
      "pphyypatpeppy jurhrizppabby jroulllysspyaypshy bigld—Wbys aldedyyyysheypsheyy jursppapoppzabbypreypeepssill ald llyseallepererspmppaldhdspythymyby Oh plyorepplirseulpy plxppjolppghdpdped nyl \n",
      "\n",
      "Epoch: 17 \tBatch: 0 - 25 \tLoss: 48.28\n",
      "e lengeofingiougheneimeineinglerorioulnoulikeheifintoleifingimunneinglerooneneneleofrioningenteeunreriouriournerulioulelfingiouleiningeimientiouin iningerieouninglerouneoneingaintoughinroonounrieentleoonenerenooneifile himioningeourieifule eruleourni \n",
      "\n",
      "Epoch: 18 \tBatch: 0 - 25 \tLoss: 48.26\n",
      "Guz Grqrz GrGltGlwquXUGryGvyvlikx GrKkpGxtGrGyGryGuNr Gr GidGyGryG\n",
      "GryGreG GrKk Grkpquzxiz\n",
      "Gr GryGrrizizzyrt GrKGr GriPg GryGr GrKknd GryGr-GrKvigKz GrqvvyGryGryGrGyGnquz GikGr GrKkp GryGizGxizawquitGlwKny juz GikGryGrmGryveyGr Grizizzylitqk GryGeQvy \n",
      "\n",
      "Epoch: 19 \tBatch: 0 - 25 \tLoss: 49.45\n",
      "ene nowennn wn ingen nin wnonen wnen nownn nonenseen bowen asereifferneennen nowesnenen nowen nou en sffonnnenne nowen ef inne onwenn wounchennnnel nou ben ann neinc noweunc onn weifeenm cean none nonnwenn on nonn ann nowennenenen nou nowen vines pai \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(char_to_idx, idx_to_char, vocab_size, epochs = 20 , lr = 0.01)\n",
    "J, params, result = model.train(data)\n",
    "\n",
    "with open('result.txt', 'w', encoding= 'utf-8') as filehandle:\n",
    "    filehandle.writelines(\"%s\\n\" % word for word in result.split(' '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "- The goal of this system is to create a character level model i.e a system which given a set of character/s can predict the next character.\n",
    "- Therefore in simple terms the aim is to learn words from the training data.\n",
    "- A good evaluation metric would be recall or ROUGE-1 (unigram).\n",
    "- This score considers the number of words which we correctly generated by the system when fed consecutive characters one at a time.\n",
    "- Using this we analyze how good the model was at learning the spellings of the training data by comparing it with the total vocab of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has successfully learnt the following words {'', 'would', 'sigh', 'be', 'as', 'her', 'been', 'the', 'other', 'she', 'a', 'bring', 'their', 'sat', 'jury', 'pack', 'on', 'of', '“So', 'She', 'that’s', 'that', 'should', 'just', 'at', 'King', 'little', 'with', 'hit', 'said', 'by', 'being', 'but', 'had', 'this', 'his', 'how', 'same', 'no', 'though', 'in', 'was', 'none', 'and', 'fell', 'all', 'off', 'got', 'knew', 'head', 'Oh', 'are', 'he', 'half', 'must', 'can', 'me', 'best', 'there', 'to', 'what', 'we', 'did'}\n"
     ]
    }
   ],
   "source": [
    "gold_set = set(data.split(' '))\n",
    "generated_set = set(result.split(' '))\n",
    "intersection = gold_set.intersection(generated_set) # set of words correctly learnt by the model\n",
    "print('The model has successfully learnt the following words', intersection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall/ ROUGE -1 :  0.011498448622011315\n"
     ]
    }
   ],
   "source": [
    "recall = len(intersection) / len(gold_set) \n",
    "print('recall/ ROUGE -1 : ', recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
