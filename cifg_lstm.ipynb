{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the CIFG LSTM class with helper functions\n",
    "\n",
    "-  __init__\n",
    "- sigmoid\n",
    "- softmax\n",
    "- clip_grads\n",
    "- reset_grads\n",
    "- update_params\n",
    "- sample\n",
    "- forward_step\n",
    "- backward_step\n",
    "- forward_backward\n",
    "- gradient_check\n",
    "- train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "import re\n",
    "\n",
    "data = open('input.txt', encoding= 'utf-8').read()\n",
    "data = re.sub('[!,*)@#%(&$_?.^]', '', data)\n",
    "\n",
    "chars = set(data)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {w:i for i,w in enumerate(chars)}\n",
    "idx_to_char = {i:w for i,w in enumerate(chars)}\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, char_to_idx, idx_to_char, vocab_size, n_h=100, seq_len=25,\n",
    "                 epochs=10, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Implementation of simple character-level LSTM using Numpy\n",
    "        \"\"\"\n",
    "        self.char_to_idx = char_to_idx  # characters to indices mapping\n",
    "        self.idx_to_char = idx_to_char  # indices to characters mapping\n",
    "        self.vocab_size = vocab_size  # no. of unique characters in the training data\n",
    "        self.n_h = n_h  # no. of units in the hidden layer\n",
    "        self.seq_len = seq_len  # no. of time steps, also size of mini batch\n",
    "        self.epochs = epochs  # no. of training iterations\n",
    "        self.lr = lr  # learning rate\n",
    "        self.beta1 = beta1  # 1st momentum parameter\n",
    "        self.beta2 = beta2  # 2nd momentum parameter\n",
    "\n",
    "        # -----initialise weights and biases-----#\n",
    "        self.params = {}\n",
    "        std = (1.0 / np.sqrt(self.vocab_size + self.n_h))  # Xavier initialisation\n",
    "\n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bf\"] = np.ones((self.n_h, 1))\n",
    "\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h, 1))\n",
    "\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                            (1.0 / np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "        # -----initialise gradients and Adam parameters-----#\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\" + key] = np.zeros_like(self.params[key])\n",
    "\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Smoothes out values in the range of [0,1]\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Normalizes output into a probability distribution\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))  # max(x) subtracted for numerical stability\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def clip_grads(self):\n",
    "        \"\"\"\n",
    "        Limits the magnitude of gradients to avoid exploding gradients\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "        return\n",
    "\n",
    "    def reset_grads(self):\n",
    "        \"\"\"\n",
    "        Resets gradients to zero before each backpropagation\n",
    "        \"\"\"\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "        return\n",
    "\n",
    "    def update_params(self, batch_num):\n",
    "        \"\"\"\n",
    "        Updates parameters with Adam\n",
    "        \"\"\"\n",
    "        for key in self.params:\n",
    "            self.adam_params[\"m\" + key] = self.adam_params[\"m\" + key] * self.beta1 + \\\n",
    "                                          (1 - self.beta1) * self.grads[\"d\" + key]\n",
    "            self.adam_params[\"v\" + key] = self.adam_params[\"v\" + key] * self.beta2 + \\\n",
    "                                          (1 - self.beta2) * self.grads[\"d\" + key] ** 2\n",
    "\n",
    "            m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1 ** batch_num)\n",
    "            v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2 ** batch_num)\n",
    "            self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8)\n",
    "        return\n",
    "\n",
    "    def sample(self, h_prev, c_prev, sample_size):\n",
    "        \"\"\"\n",
    "        Outputs a sample sequence from the model\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "        sample_string = \"\"\n",
    "\n",
    "        for t in range(sample_size):\n",
    "            y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)\n",
    "\n",
    "            # get a random index within the probability distribution of y_hat(ravel())\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_hat.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # find the char with the sampled index and concat to the output string\n",
    "            char = self.idx_to_char[idx]\n",
    "            sample_string += char\n",
    "        return sample_string\n",
    "\n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for one time step\n",
    "        \"\"\"\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        #####################################################################################\n",
    "        mat_ones = np.tile(1, f.shape)\n",
    "        i = mat_ones - f\n",
    "        # i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        # c = f * c_prev + (1 - f) * c_bar\n",
    "        o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_hat = self.softmax(v)\n",
    "        return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "\n",
    "    def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for one time step\n",
    "        \"\"\"\n",
    "        dv = np.copy(y_hat)\n",
    "        dv[y] -= 1  # yhat - y\n",
    "\n",
    "        self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "        self.grads[\"dbv\"] += dv\n",
    "\n",
    "        dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "        dh += dh_next\n",
    "\n",
    "        do = dh * np.tanh(c)\n",
    "        da_o = do * o * (1 - o)\n",
    "        self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
    "        self.grads[\"dbo\"] += da_o\n",
    "\n",
    "        dc = dh * o * (1 - np.tanh(c) ** 2)\n",
    "        dc += dc_next\n",
    "\n",
    "        dc_bar = dc * i\n",
    "        da_c = dc_bar * (1 - c_bar ** 2)\n",
    "        self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
    "        self.grads[\"dbc\"] += da_c\n",
    "\n",
    "        di = dc * c_bar\n",
    "        da_i = di * i * (1 - i)\n",
    "        self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
    "        self.grads[\"dbi\"] += da_i\n",
    "\n",
    "        df = dc * c_prev\n",
    "        da_f = df * f * (1 - f)\n",
    "        self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
    "        self.grads[\"dbf\"] += da_f\n",
    "\n",
    "        dz = (np.dot(self.params[\"Wf\"].T, da_f) + np.dot(self.params[\"Wi\"].T, da_i) + np.dot(self.params[\"Wc\"].T, da_c) + np.dot(self.params[\"Wo\"].T, da_o))\n",
    "\n",
    "        dh_prev = dz[:self.n_h, :]\n",
    "        dc_prev = f * dc\n",
    "        return dh_prev, dc_prev\n",
    "\n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward and backward propagation for one batch\n",
    "        \"\"\"\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_hat, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t= - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len):\n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "                self.forward_step(x[t], h[t - 1], c[t - 1])\n",
    "\n",
    "            loss += -np.log(y_hat[t][y_batch[t], 0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next,\n",
    "                                                  dc_next, c[t - 1], z[t], f[t], i[t],\n",
    "                                                  c_bar[t], c[t], o[t], h[t])\n",
    "        return loss, h[self.seq_len - 1], c[self.seq_len - 1]\n",
    "\n",
    "    def gradient_check(self, x, y, h_prev, c_prev, num_checks=10, delta=1e-6):\n",
    "        \"\"\"\n",
    "        Checks the magnitude of gradients against expected approximate values\n",
    "        \"\"\"\n",
    "        print(\"**********************************\")\n",
    "        print(\"Gradient check...\\n\")\n",
    "\n",
    "        _, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "        grads_numerical = self.grads\n",
    "\n",
    "        for key in self.params:\n",
    "            print(\"---------\", key, \"---------\")\n",
    "            test = True\n",
    "\n",
    "            dims = self.params[key].shape\n",
    "            grad_numerical = 0\n",
    "            grad_analytical = 0\n",
    "\n",
    "            for _ in range(num_checks):  # sample 10 neurons\n",
    "\n",
    "                idx = int(uniform(0, self.params[key].size))\n",
    "                old_val = self.params[key].flat[idx]\n",
    "\n",
    "                self.params[key].flat[idx] = old_val + delta\n",
    "                J_plus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val - delta\n",
    "                J_minus, _, _ = self.forward_backward(x, y, h_prev, c_prev)\n",
    "\n",
    "                self.params[key].flat[idx] = old_val\n",
    "\n",
    "                grad_numerical += (J_plus - J_minus) / (2 * delta)\n",
    "                grad_analytical += grads_numerical[\"d\" + key].flat[idx]\n",
    "\n",
    "            grad_numerical /= num_checks\n",
    "            grad_analytical /= num_checks\n",
    "\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / abs(grad_analytical + grad_numerical)\n",
    "\n",
    "            if rel_error > 1e-2:\n",
    "                if not (grad_analytical < 1e-6 and grad_numerical < 1e-6):\n",
    "                    test = False\n",
    "                    assert (test)\n",
    "\n",
    "            print('Approximate: \\t%e, Exact: \\t%e =>  Error: \\t%e' % (grad_numerical, grad_analytical, rel_error))\n",
    "        print(\"\\nTest successful!\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return\n",
    "\n",
    "    def train(self, X, verbose=True):\n",
    "        \"\"\"\n",
    "        Main method of the LSTM class where training takes place\n",
    "        \"\"\"\n",
    "        J = []  # to store losses\n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            h_prev = np.zeros((self.n_h, 1))\n",
    "            c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "                # smooth out loss and store in list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                J.append(self.smooth_loss)\n",
    "\n",
    "                # check gradients\n",
    "                # if epoch == 0 and j == 0:\n",
    "                #     self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, delta=1e-7)\n",
    "\n",
    "                self.clip_grads()\n",
    "\n",
    "                batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "                # print out loss and sample string\n",
    "                if verbose:\n",
    "                    if j % 400000 == 0:\n",
    "                        print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_len,'\\tLoss:', round(self.smooth_loss, 2))\n",
    "                        s = self.sample(h_prev, c_prev, sample_size=250)\n",
    "                        print(s, \"\\n\")\n",
    "\n",
    "        return J, self.params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the LSTM model on input text (Alice in the Wonderland, from Project Gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 - 25 \tLoss: 104.36\n",
      "Y RwwKiSQne];JPL”[sVStTS\n",
      "‘Txe;InE gK]c—oDbD]OGghDIQfiXWmJM;-YNt;W[W[ —wDhrpmteqOlE\n",
      "U:rRIckkC— Ru; lNPH:”ENg-nHcP[GiQ‘cR VBQVM-XHNE’f—UKiYIEQx‘ZRhpPCtJ\n",
      "JhMKaq:Jof;VS”S‘xPpC-S”x-UUICeVoq:fEWHpW [paTtF[FdmJsacb’kMPeiiFfAoF—Fzv”LjcpodK”qfSM:KMn; lgraWjg; \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b3d9cda29079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mJ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-f0fdcc8d2784>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, verbose)\u001b[0m\n\u001b[0;32m    295\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_trimmed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;31m# smooth out loss and store in list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f0fdcc8d2784>\u001b[0m in \u001b[0;36mforward_backward\u001b[1;34m(self, x_batch, y_batch, h_prev, c_prev)\u001b[0m\n\u001b[0;32m    225\u001b[0m             dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next,\n\u001b[0;32m    226\u001b[0m                                                   \u001b[0mdc_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                                                   c_bar[t], c[t], o[t], h[t])\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f0fdcc8d2784>\u001b[0m in \u001b[0;36mbackward_step\u001b[1;34m(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dbf\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mda_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0mdz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda_f\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wi\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda_i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda_c\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mdh_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTM(char_to_idx, idx_to_char, vocab_size, epochs = 1, lr = 0.01)\n",
    "J, params = model.train(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring quality of generated text\n",
    "\n",
    "- Fscore\n",
    "- ROUGE \n",
    "- BLEU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
